{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4af6cf",
   "metadata": {
    "id": "zAXHC6-Tn2O5",
    "papermill": {
     "duration": 0.00462,
     "end_time": "2023-05-27T00:43:16.590977",
     "exception": false,
     "start_time": "2023-05-27T00:43:16.586357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21785289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:16.599954Z",
     "iopub.status.busy": "2023-05-27T00:43:16.599253Z",
     "iopub.status.idle": "2023-05-27T00:43:17.692431Z",
     "shell.execute_reply": "2023-05-27T00:43:17.691523Z"
    },
    "papermill": {
     "duration": 1.100422,
     "end_time": "2023-05-27T00:43:17.694942",
     "exception": false,
     "start_time": "2023-05-27T00:43:16.594520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import jo_wilder\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469a83b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:17.703229Z",
     "iopub.status.busy": "2023-05-27T00:43:17.702672Z",
     "iopub.status.idle": "2023-05-27T00:43:19.796408Z",
     "shell.execute_reply": "2023-05-27T00:43:19.795313Z"
    },
    "papermill": {
     "duration": 2.100948,
     "end_time": "2023-05-27T00:43:19.799266",
     "exception": false,
     "start_time": "2023-05-27T00:43:17.698318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "model_folder = '/kaggle/input/model-may26'\n",
    "\n",
    "models = {}\n",
    "\n",
    "for file in os.listdir(model_folder):\n",
    "    \n",
    "    if 'question' in file:\n",
    "        x = int(file.split('_')[1])\n",
    "        \n",
    "        with open(f'{model_folder}/{file}', 'rb') as file:\n",
    "            model_ = pickle.load(file)\n",
    "        \n",
    "        models[f'question_{x}'] = model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f4d611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:19.809366Z",
     "iopub.status.busy": "2023-05-27T00:43:19.808431Z",
     "iopub.status.idle": "2023-05-27T00:43:19.815896Z",
     "shell.execute_reply": "2023-05-27T00:43:19.815112Z"
    },
    "papermill": {
     "duration": 0.014664,
     "end_time": "2023-05-27T00:43:19.818037",
     "exception": false,
     "start_time": "2023-05-27T00:43:19.803373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def kaggle_prep(test_data=None):\n",
    "#     \"how to prep the data for the kaggle notebook\"\n",
    "    \n",
    "#     level_dict = {\n",
    "#         '0-4' : [1, 2, 3],\n",
    "#         '5-12' : [4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "#         '13-22' : [14, 15, 16, 17, 18]\n",
    "#     }\n",
    "\n",
    "    \n",
    "#     df_ = test_data.copy()\n",
    "#         # getting elapsed diffs\n",
    "#     df_['event_time_delta'] = (\n",
    "#         df_\n",
    "#         .groupby('session_id')['elapsed_time']\n",
    "#         .transform(lambda x: x.diff().fillna(0))\n",
    "#     )\n",
    "\n",
    "#     # getting the time until the next event\n",
    "#     df_['time_delta_til_next'] = (\n",
    "#         df_\n",
    "#         .groupby('session_id')['elapsed_time']\n",
    "#         .transform(lambda x: abs(x.diff(-1)).fillna(0)))\n",
    "    \n",
    "    \n",
    "#     # time delta means    \n",
    "#     time_delta_mean = df_.groupby('session_id').agg(event_time_mean=('event_time_delta', 'mean'),\n",
    "#                                                     event_time_std=('event_time_delta', 'std'),\n",
    "#                                                     event_time_max=('event_time_delta', 'max'))\n",
    "    \n",
    "#     # hover duration stats\n",
    "#     hover_duration = df_.groupby('session_id').agg(hover_duration_mean=('hover_duration', 'mean'),\n",
    "#                                                         hover_duration_std=('hover_duration', 'std'),\n",
    "#                                                         hover_duration_max=('hover_duration', 'max'))\n",
    "    \n",
    "#     # total time on each event\n",
    "#     total_time_event = (\n",
    "#         df_\n",
    "#         .groupby(['session_id', 'event_name'])\n",
    "#         .agg(total_time_event=('time_delta_til_next', 'sum'),\n",
    "#              mean_time_event=('time_delta_til_next', 'mean'),\n",
    "#              std_time_event=('time_delta_til_next', 'std'))\n",
    "#         .unstack()\n",
    "#     )\n",
    "    \n",
    "#     # total time on each level    \n",
    "#     level_duration = (\n",
    "#         df_\n",
    "#         .groupby(['session_id', 'level'])['elapsed_time']\n",
    "#         .apply(lambda x: x.max() - x.min())\n",
    "#         .unstack()\n",
    "#         .rename(columns=lambda x: f'time_on_level_{x}')\n",
    "#     )\n",
    "    \n",
    "#     # unique text ids\n",
    "#     nunique_text_ids = df_.groupby('session_id')['text_fqid'].nunique()\n",
    "    \n",
    "#     # unique fqids \n",
    "#     nunique_fqid = df_.groupby('session_id')['fqid'].nunique()\n",
    "    \n",
    "#     # how many events of each type occurred in the session\n",
    "#     event_counts = df_.groupby('session_id')['event_name'].value_counts().unstack()\n",
    "    \n",
    "#     # getting session lengths\n",
    "#     session_lengths = df_.groupby('session_id')['elapsed_time'].max().rename('total_length')\n",
    "    \n",
    "#     # total events in the session\n",
    "#     session_events = df_.groupby('session_id')['session_id'].count()\n",
    "    \n",
    "#     # final df pre labels\n",
    "#     df_features = (\n",
    "#         pd.concat([total_time_event, nunique_text_ids, nunique_fqid, event_counts, \n",
    "#                    session_lengths, session_events, time_delta_mean, hover_duration,\n",
    "#                    level_duration], axis=1)\n",
    "#         .fillna(0)\n",
    "#         .drop(columns='session_id')\n",
    "#         .reset_index()\n",
    "#     )\n",
    "    \n",
    "#     # fixing column names \n",
    "#     df_features.rename(columns=lambda x: '_'.join(x) if isinstance(x, tuple) else x, inplace=True)\n",
    "    \n",
    "#     return df_features.set_index('session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5aeed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:19.827552Z",
     "iopub.status.busy": "2023-05-27T00:43:19.826698Z",
     "iopub.status.idle": "2023-05-27T00:43:19.850755Z",
     "shell.execute_reply": "2023-05-27T00:43:19.849943Z"
    },
    "papermill": {
     "duration": 0.031131,
     "end_time": "2023-05-27T00:43:19.852980",
     "exception": false,
     "start_time": "2023-05-27T00:43:19.821849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kaggle_prep(test_data=None):\n",
    "    \"how to prep the data for the kaggle notebook\"\n",
    "    \n",
    "    level_dict = {\n",
    "        '0-4' : [1, 2, 3],\n",
    "        '5-12' : [4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "        '13-22' : [14, 15, 16, 17, 18]\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_ = test_data.copy()\n",
    "   \n",
    "    # getting elapsed diffs\n",
    "    df_['event_time_delta'] = (\n",
    "        df_\n",
    "        .groupby('session_id')['elapsed_time']\n",
    "        .transform(lambda x: x.diff().fillna(0))\n",
    "        .clip(0, 103000) \n",
    "    )\n",
    "\n",
    "    # getting the time until the next event\n",
    "    df_['time_delta_til_next'] = (\n",
    "        df_\n",
    "        .groupby('session_id')['elapsed_time']\n",
    "        .transform(lambda x: abs(x.diff(-1)).fillna(0))\n",
    "        .clip(0, 103000)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # time delta means    \n",
    "    time_delta_mean = df_.groupby('session_id').agg(event_time_mean=('event_time_delta', 'mean'),\n",
    "                                                    event_time_std=('event_time_delta', 'std'),\n",
    "                                                    event_time_max=('event_time_delta', 'max'))\n",
    "    \n",
    "    # hover duration stats\n",
    "    hover_duration = df_.groupby('session_id').agg(hover_duration_mean=('hover_duration', 'mean'),\n",
    "                                                        hover_duration_std=('hover_duration', 'std'),\n",
    "                                                        hover_duration_max=('hover_duration', 'max'))\n",
    "    \n",
    "    # total time on each event\n",
    "    total_time_event = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'event_name'])\n",
    "        .agg(total_time_event=('time_delta_til_next', 'sum'),\n",
    "             mean_time_event=('time_delta_til_next', 'mean'),\n",
    "             std_time_event=('time_delta_til_next', 'std'))\n",
    "        .unstack()\n",
    "    )\n",
    "    \n",
    "    # total time on text_id\n",
    "    total_time_text = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'text_fqid'])['event_time_delta']\n",
    "        .sum()\n",
    "        .unstack()\n",
    "        .rename(columns=lambda x: f\"time_on_{x}\")\n",
    "    )\n",
    "    \n",
    "    # total time on each level    \n",
    "    level_duration = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'level'])['elapsed_time']\n",
    "        .apply(lambda x: x.max() - x.min())\n",
    "        .unstack()\n",
    "        .rename(columns=lambda x: f'time_on_level_{x}')\n",
    "    )\n",
    "    \n",
    "    # page counts\n",
    "    page_counts = df_.groupby('session_id')['page'].value_counts().unstack().fillna(0)\n",
    "    page_counts.columns = [f'page_{x}' for x in page_counts.columns]\n",
    "    \n",
    "    # unique rooms\n",
    "    nunique_rooms = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'level'])['room_fqid']\n",
    "        .nunique()\n",
    "        .unstack()\n",
    "        .rename(columns=lambda x: f'unique_room_fqid_level_{x}')\n",
    "    )\n",
    "    \n",
    "    # unique text ids\n",
    "    nunique_text_ids = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'level'])['text_fqid']\n",
    "        .nunique()\n",
    "        .unstack()\n",
    "        .rename(columns=lambda x: f'unique_text_fqid_level_{x}')\n",
    "    )\n",
    "    \n",
    "    # unique fqids \n",
    "    nunique_fqid = (\n",
    "        df_\n",
    "        .groupby(['session_id', 'level'])['fqid']\n",
    "        .nunique()\n",
    "        .unstack()\n",
    "        .rename(columns=lambda x: f'unique_fqid_level_{x}')\n",
    "    )\n",
    "    \n",
    "    # how many events of each type occurred in the session\n",
    "    event_counts = df_.groupby('session_id')['event_name'].value_counts().unstack()\n",
    "    \n",
    "    # getting session lengths\n",
    "    session_lengths = df_.groupby('session_id')['elapsed_time'].max().rename('total_length')\n",
    "    \n",
    "    # total events in the session\n",
    "    session_events = df_.groupby('session_id')['session_id'].count()\n",
    "    \n",
    "    # final df pre labels\n",
    "    df_features = (\n",
    "        pd.concat([total_time_event, nunique_text_ids, nunique_fqid, event_counts, \n",
    "                   session_lengths, session_events, time_delta_mean, hover_duration,\n",
    "                   level_duration, page_counts, total_time_text], axis=1)\n",
    "        .fillna(0)\n",
    "        .drop(columns='session_id')\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # fixing column names \n",
    "    df_features.rename(columns=lambda x: '_'.join(x) if isinstance(x, tuple) else x, inplace=True)\n",
    "    \n",
    "    return df_features.set_index('session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c83a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:19.862011Z",
     "iopub.status.busy": "2023-05-27T00:43:19.861166Z",
     "iopub.status.idle": "2023-05-27T00:43:19.865692Z",
     "shell.execute_reply": "2023-05-27T00:43:19.864881Z"
    },
    "papermill": {
     "duration": 0.011777,
     "end_time": "2023-05-27T00:43:19.868269",
     "exception": false,
     "start_time": "2023-05-27T00:43:19.856492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = jo_wilder.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7187f1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-27T00:43:19.876711Z",
     "iopub.status.busy": "2023-05-27T00:43:19.876411Z",
     "iopub.status.idle": "2023-05-27T00:43:20.621660Z",
     "shell.execute_reply": "2023-05-27T00:43:20.620597Z"
    },
    "papermill": {
     "duration": 0.752782,
     "end_time": "2023-05-27T00:43:20.624573",
     "exception": false,
     "start_time": "2023-05-27T00:43:19.871791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    }
   ],
   "source": [
    "limits = {'0-4':(1,4), '5-12':(4,14), '13-22':(14,19)}\n",
    "\n",
    "for test, sample_submission in iter_test:\n",
    "    \n",
    "    level_group = test['level_group'].values[0]\n",
    "    df_ = kaggle_prep(test_data=test)\n",
    "     \n",
    "    if level_group == '0-4':\n",
    "        feature_cols = models['question_1'].feature_name_\n",
    "    elif level_group == '5-12':\n",
    "        feature_cols = models['question_6'].feature_name_\n",
    "    else:\n",
    "        feature_cols = models['question_17'].feature_name_\n",
    "    \n",
    "    # figure out if feature columns are missing - important for submission\n",
    "    missing_columns = list(set(feature_cols) - set(df_.columns))\n",
    "    \n",
    "    # create a dataframe with the missing columns filled with 0s (for now)\n",
    "    missing_df = pd.DataFrame(0, columns=missing_columns, index=df_.index)\n",
    "    \n",
    "    # combine the original df and the missing df\n",
    "    df_ = pd.concat([df_, missing_df], axis=1)\n",
    "    \n",
    "    a, b = limits[level_group]\n",
    "    \n",
    "    for question in range(a, b):\n",
    "        model_ = models[f'question_{question}']\n",
    "        threshold = 0.63\n",
    "        feature_cols = model_.feature_name_\n",
    "\n",
    "        preds = model_.predict_proba(df_[feature_cols])[0, 1]\n",
    "        fixed_preds = (preds > threshold).astype(int)\n",
    "\n",
    "        mask = sample_submission.session_id.str.contains(f'q{question}')\n",
    "        sample_submission.loc[mask, 'correct'] = fixed_preds\n",
    "\n",
    "    env.predict(sample_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.419748,
   "end_time": "2023-05-27T00:43:21.351411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-27T00:43:04.931663",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
